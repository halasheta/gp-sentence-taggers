{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd6a9b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, TimeDistributed\n",
    "from tensorflow.keras.models import Sequential\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import brown\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import brown, treebank\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "import os\n",
    "path = api.load(\"word2vec-google-news-300\", return_path=True)\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "371e98fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/annielin/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/annielin/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/annielin/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a63e78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = brown.tagged_words(tagset='universal')\n",
    "txt2 = treebank.tagged_words(tagset='universal')\n",
    "brown_sent = brown.tagged_sents(tagset='universal')\n",
    "tree_sent = treebank.tagged_sents(tagset='universal')\n",
    "# all_sent = brown_sent + tree_sent\n",
    "all_sent = brown_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ce1a7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the words\n",
    "# w_tokenizer = Tokenizer()\n",
    "# pos_tokenizer = Tokenizer()\n",
    "# tokens = [(tup[0].lower(), tup[1]) for tup in txt]\n",
    "# tokens.extend([(tup[0].lower(), tup[1]) for tup in txt2])\n",
    "\n",
    "word = [word[0].lower() for tup in all_sent for word in tup] # store the word\n",
    "pos = [pos[1].lower() for tup in all_sent for pos in tup] # store the corresponding pos tag\n",
    "\n",
    "word_tokenizer = Tokenizer()\n",
    "pos_tokenizer = Tokenizer()\n",
    "\n",
    "word_tokenizer.fit_on_texts(word) \n",
    "word_seqs = word_tokenizer.texts_to_sequences(word)  \n",
    "pos_tokenizer.fit_on_texts(pos)\n",
    "pos_seqs = pos_tokenizer.texts_to_sequences(pos)\n",
    "\n",
    "# word_vectors = KeyedVectors.load_word2vec_format('path/to/word2vec.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bbcb25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the length of the training sets\n",
    "max_len = 100 # what do we want as the max length of padding for sequence?\n",
    "w_size = len(word_tokenizer.word_index) + 1\n",
    "pos_size = len(pos_tokenizer.word_index) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "371cc75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code modified from https://towardsdatascience.com/pos-tagging-using-rnn-7f08a522f849\n",
    "word_vectors = KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "embedding_size = 300  \n",
    "embedding_weights = np.zeros((w_size, embedding_size))\n",
    "word2id = word_tokenizer.word_index\n",
    "for word, index in word2id.items():\n",
    "    try:\n",
    "        embedding_weights[index, :] = word_vectors[word]\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7091614c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 100, 300)          13362600  \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100, 64)           93440     \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 100, 12)          780       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,456,820\n",
      "Trainable params: 94,220\n",
      "Non-trainable params: 13,362,600\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# the LSTM Model\n",
    "model = Sequential()\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "\n",
    "model.add(Embedding(w_size, embedding_size, input_length=max_len, weights=[embedding_weights], trainable=False)) \n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(pos_size, activation='softmax')))\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3329d595",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1114/1114 [==============================] - 14881s 13s/step - loss: 0.0064 - accuracy: 0.9981 - val_loss: 0.0061 - val_accuracy: 0.9981\n",
      "35174/35174 [==============================] - 13015s 370ms/step - loss: 0.0061 - accuracy: 0.9981\n"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "\n",
    "# split the data for better training\n",
    "split_idx = int(0.8 * w_size)\n",
    "word_train, pos_train = word_seqs[:split_idx], pos_seqs[:split_idx]\n",
    "word_test, pos_test = word_seqs[split_idx:], pos_seqs[split_idx:]\n",
    "\n",
    "# pad the data\n",
    "word_train = pad_sequences(word_train, max_len, padding='post', truncating='post') \n",
    "pos_train = pad_sequences(pos_train, max_len, padding='post', truncating='post')\n",
    "word_test = pad_sequences(word_test, maxlen=max_len, padding='post', truncating='post')\n",
    "pos_test = pad_sequences(pos_test, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# convert pos data set into one-hot encoding\n",
    "pos_train = to_categorical(pos_train, num_classes=pos_size)\n",
    "pos_test = to_categorical(pos_test, num_classes=pos_size)\n",
    "\n",
    "training = model.fit(word_train, pos_train, batch_size=32, epochs=1, validation_data=(word_test, pos_test))\n",
    "loss, accuracy = model.evaluate(word_test, pos_test)\n",
    "model.save('lstm_model.h5')\n",
    "print('Model Done')\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80ee8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03ba26e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5768d016",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  right now the model is trained with only brown data set for faster runtime\n",
    "#  may increase the data with treebank for better training in future"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
