{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd6a9b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 13:35:11.859379: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, TimeDistributed\n",
    "from tensorflow.keras.models import Sequential\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import brown\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import brown, treebank\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "from nltk.corpus import conll2000\n",
    "import os\n",
    "path = api.load(\"word2vec-google-news-300\", return_path=True)\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "371e98fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/annielin/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/annielin/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/annielin/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('treebank')\n",
    "nltk.download('conll2000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a63e78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = brown.tagged_words(tagset='universal')\n",
    "txt2 = treebank.tagged_words(tagset='universal')\n",
    "brown_sent = brown.tagged_sents(tagset='universal')\n",
    "tree_sent = treebank.tagged_sents(tagset='universal')\n",
    "conll_sent = conll2000.tagged_sents(tagset='universal')\n",
    "all_sent = brown_sent + tree_sent + conll_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ce1a7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the words\n",
    "\n",
    "word = [word[0].lower() for tup in all_sent for word in tup] # store the word\n",
    "pos = [pos[1].lower() for tup in all_sent for pos in tup] # store the corresponding pos tag\n",
    "\n",
    "word_tokenizer = Tokenizer()\n",
    "pos_tokenizer = Tokenizer()\n",
    "\n",
    "word_tokenizer.fit_on_texts(word) \n",
    "word_seqs = word_tokenizer.texts_to_sequences(word)  \n",
    "pos_tokenizer.fit_on_texts(pos)\n",
    "pos_seqs = pos_tokenizer.texts_to_sequences(pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bbcb25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the length of the training sets\n",
    "max_len = 1 # don't need to pad fixed len dataset\n",
    "w_size = len(word_tokenizer.word_index) + 1\n",
    "pos_size = len(pos_tokenizer.word_index) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "371cc75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code modified from https://towardsdatascience.com/pos-tagging-using-rnn-7f08a522f849\n",
    "word_vectors = KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "embedding_size = 300  \n",
    "embedding_weights = np.zeros((w_size, embedding_size))\n",
    "word2id = word_tokenizer.word_index\n",
    "for word, index in word2id.items():\n",
    "    try:\n",
    "        embedding_weights[index, :] = word_vectors[word]\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7091614c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-23 23:50:24.330887: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 300)          13362600  \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100, 64)           93440     \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 100, 12)          780       \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,456,820\n",
      "Trainable params: 94,220\n",
      "Non-trainable params: 13,362,600\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# the LSTM Model\n",
    "model = Sequential()\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "model.add(Embedding(w_size, embedding_size, input_length=max_len, weights=[embedding_weights], trainable=False)) \n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(pos_size, activation='softmax')))\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3329d595",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annielin/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/ops/structured_function.py:256: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1114/1114 [==============================] - 4225s 4s/step - loss: 0.0151 - accuracy: 0.9971 - val_loss: 0.0061 - val_accuracy: 0.9981\n",
      "35174/35174 [==============================] - 4272s 121ms/step - loss: 0.0061 - accuracy: 0.9981\n",
      "Model Done\n"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "\n",
    "# split the data for better training\n",
    "\n",
    "split_idx = int(0.8 * w_size)\n",
    "word_train, pos_train = word_seqs[:split_idx], pos_seqs[:split_idx]\n",
    "word_test, pos_test = word_seqs[split_idx:], pos_seqs[split_idx:]\n",
    "\n",
    "# pad the data\n",
    "# word_train = pad_sequences(word_train, max_len, padding='post', truncating='post') \n",
    "# pos_train = pad_sequences(pos_train, max_len, padding='post', truncating='post')\n",
    "# word_test = pad_sequences(word_test, maxlen=max_len, padding='post', truncating='post')\n",
    "# pos_test = pad_sequences(pos_test, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# convert pos data set into one-hot encoding\n",
    "pos_train = to_categorical(pos_train, num_classes=pos_size)\n",
    "pos_test = to_categorical(pos_test, num_classes=pos_size)\n",
    "\n",
    "training = model.fit(word_train, pos_train, batch_size=32, epochs=10, validation_data=(word_test, pos_test))\n",
    "loss, accuracy = model.evaluate(word_test, pos_test)\n",
    "model.save('lstm_model.h5')\n",
    "print('Model Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80ee8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03ba26e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5768d016",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  right now the model is trained with only brown data set for faster runtime\n",
    "#  may increase the data with treebank for better training in future"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
