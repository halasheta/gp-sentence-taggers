{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dd6a9b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import brown\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "371e98fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/annielin/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/annielin/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "txt = brown.tagged_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ce1a7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the words\n",
    "w_tokenizer = Tokenizer()\n",
    "pos_tokenizer = Tokenizer()\n",
    "tokens = [(tup[0].lower(), tup[1]) for tup in txt]\n",
    "\n",
    "word_freq = {}\n",
    "# Make a word-frequency dict\n",
    "for token in tokens:\n",
    "    if token[0] not in word_freq:\n",
    "        word_freq[token[0]] = {}\n",
    "        \n",
    "    if token[1] not in word_freq[token[0]]:\n",
    "        word_freq[token[0]][token[1]] = 1\n",
    "    else:\n",
    "        word_freq[token[0]][token[1]] += 1\n",
    "        \n",
    "w_tokenizer.fit_on_texts([t[0] for t in tokens])\n",
    "pos_tokenizer.fit_on_texts([t[1] for t in tokens])\n",
    "\n",
    "w_seqs = w_tokenizer.texts_to_sequences([t[0] for t in tokens])\n",
    "pos_seqs = pos_tokenizer.texts_to_sequences([t[1] for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c0f497ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad the sentences\n",
    "max_len = 50\n",
    "pos_size = len(pos_tokenizer.word_index) + 1\n",
    "w_seqs = pad_sequences(w_seqs, max_len, padding='post', truncating='post') # what do we want as the max length of padding for sequence?\n",
    "pos_seqs = pad_sequences(pos_seqs, max_len, padding='post', truncating='post')\n",
    "pos_encoded = to_categorical(pos_seqs, num_classes=pos_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7091614c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 50, 50)            2227100   \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 10)                2440      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 79)                869       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,230,409\n",
      "Trainable params: 2,230,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# the LSTM layer\n",
    "model = keras.Sequential()\n",
    "data_size = len(w_tokenizer.word_index) + 1\n",
    "model.add(layers.Embedding(data_size, 50, input_length=max_len)) # may need to change the output dim\n",
    "model.add(layers.LSTM(10))\n",
    "model.add(layers.Dense(pos_size, 'softmax'))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf1e8f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
